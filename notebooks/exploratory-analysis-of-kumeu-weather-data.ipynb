{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis and model building\n",
    "\n",
    "Showing some summary graphs and statistics for the Kumeu Weather data that will be used for weather forecasting. Historical hourly weather data is available from Jan 2016 til May 2021.\n",
    "\n",
    "Goals to achieve:\n",
    "1. Predict temperature\n",
    "2. Export temperature model\n",
    "3. Repeat steps 1 and 2 with variables: rainfall, humidity, windspeed\n",
    "\n",
    "Task list:\n",
    "- ✓ Import historic data\n",
    "- ✓ Plot graphs to view data\n",
    "- ✓ Format and export historic data as csv\n",
    "- ✓ Format and export historic data as DataFrame (in pickle format) \n",
    "- ✓ Encode and standardise data to prepare for model training\n",
    "- ✓ Export standardised model data as csv and DataFrame (pickle format)\n",
    "- To do: Build model with correct parameters\n",
    "- To do: Train model\n",
    "- To do: Evaluate model\n",
    "- <b> At this stage, the model is ready to be exported as an initial proof of concept for the prediction app</b>\n",
    "- To do: Build, train and evaluate models with different architectures for model selection\n",
    "- To do: Hyper-parameter tuning to improve the selected model\n",
    "- To do: Critically analyse the strengths and weaknesses of the model and predictions\n",
    "- To do: Repeat all steps for the variables: rainfall, humidity, wind-speed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminaries: Install and import necessary libraries\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "%%capture\n",
    "# Install and import libraries. %%capture is used to suppress messages for library install statuses\n",
    "\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "import os, random, csv, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import tensorflow as tf\n",
    "#from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding, Activation, RepeatVector, TimeDistributed\n",
    "#from keras.optimizers import Adam\n",
    "\n",
    "# Suppress Tensorflow warning messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "\n",
    "# displays full numbers in dataframes instead of scientific notation for clarity\n",
    "pd.set_option('float_format', '{:f}'.format)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# For model building/training\n",
    "tf.config.experimental_run_functions_eagerly(True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import historic hourly Kumeu weather data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "#data_dir = \"../../data/Weather Station Hourly/\"\n",
    "data_dir = \"../data/Weather Station Hourly/\"\n",
    "\n",
    "input_data_paths = [data_dir+\"MetWatch Export (20{}).csv\".format(i) for i in range(16,22)]\n",
    "input_data_paths"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "['../data/Weather Station Hourly/MetWatch Export (2016).csv',\n '../data/Weather Station Hourly/MetWatch Export (2017).csv',\n '../data/Weather Station Hourly/MetWatch Export (2018).csv',\n '../data/Weather Station Hourly/MetWatch Export (2019).csv',\n '../data/Weather Station Hourly/MetWatch Export (2020).csv',\n '../data/Weather Station Hourly/MetWatch Export (2021).csv']"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "input_data = []\n",
    "\n",
    "for data_path in input_data_paths:\n",
    "    csv_list = []\n",
    "    with open(data_path) as f:\n",
    "        next(f)\n",
    "        next(f)\n",
    "        for line in f.readlines():\n",
    "            if ',' in line:\n",
    "                input_data.append(line.strip().split(','))\n",
    "\n",
    "for i in range(len(input_data)):\n",
    "    if len(input_data[i])<2:\n",
    "        input_data[i] = None\n",
    "        continue\n",
    "    else:\n",
    "        date_split = input_data[i][0].split(' ')[1:]\n",
    "        date_split[0] = re.sub('[a-zA-Z]+','',date_split[0])\n",
    "        date_split[-1] = date_split[-1].strip('\"')\n",
    "        input_data[i][0] = \" \".join(date_split)\n",
    "        if len(input_data[i])>1:\n",
    "            input_data[i][1] = input_data[i][1].strip('\"')\n",
    "            input_data[i][8] = input_data[i][8].strip('\"')\n",
    "        for j in range(1,len(input_data[i])):\n",
    "            if input_data[i][j] == '':\n",
    "                input_data[i][j] = None\n",
    "            elif input_data[i][j] == '-':\n",
    "                input_data[i][j] = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "Import data as Data Frame, tidy up column names and null values in data. Convert time series data into DateTime formats (day, month, year, hour)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "raw_df = pd.DataFrame(data=input_data, columns=['date','time_hourly','temp_C','rain_mm','wetness_percent','rel_humidity_percent',\n",
    "                                 'wind_speed_kmph','wind_direction','spray_drift_risk','backup_data','missing_data'])\n",
    "\n",
    "raw_df = raw_df.astype(dtype={'date':'object','time_hourly':'object','temp_C':'float64','rain_mm':'float64',\n",
    "                                  'wetness_percent':'float64','rel_humidity_percent':'float64',\n",
    "                                  'wind_speed_kmph':'float64','wind_direction':'object','spray_drift_risk':'object',\n",
    "                                  'backup_data':'float64','missing_data':'object'})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "raw_df['max_hour_str'] = raw_df['time_hourly'].str.split('-').str[-1].str.strip() \n",
    "raw_df['datetime_mid']= pd.to_datetime(raw_df['date'] + raw_df['max_hour_str'], format='%d %b %Y%I%p') - pd.Timedelta(minutes=30)\n",
    "raw_df['end_hour'] = pd.to_datetime(raw_df['max_hour_str'], format='%I%p').dt.hour\n",
    "raw_df['start_hour'] = raw_df['end_hour'] - 1\n",
    "raw_df['day'] = raw_df['datetime_mid'].dt.day\n",
    "raw_df['month'] = raw_df['datetime_mid'].dt.month\n",
    "raw_df['year'] = raw_df['datetime_mid'].dt.year\n",
    "\n",
    "train_df = raw_df[['datetime_mid','temp_C','rain_mm','wetness_percent','rel_humidity_percent','wind_speed_kmph','wind_direction','spray_drift_risk','backup_data','missing_data','year','month','day','start_hour','end_hour']]\n",
    "train_df = train_df.set_index('datetime_mid')\n",
    "train_df.sample(2)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "                       temp_C  rain_mm  wetness_percent  rel_humidity_percent  \\\ndatetime_mid                                                                    \n2018-02-11 03:30:00 21.400000 0.000000        99.000000            100.000000   \n2018-06-14 05:30:00 10.200000 0.000000        97.300000             99.200000   \n\n                     wind_speed_kmph wind_direction spray_drift_risk  \\\ndatetime_mid                                                           \n2018-02-11 03:30:00        13.900000             NE  Best Conditions   \n2018-06-14 05:30:00         0.000000             SW  Not Recommended   \n\n                     backup_data missing_data  year  month  day  start_hour  \\\ndatetime_mid                                                                  \n2018-02-11 03:30:00          NaN         None  2018      2   11           3   \n2018-06-14 05:30:00          NaN         None  2018      6   14           5   \n\n                     end_hour  \ndatetime_mid                   \n2018-02-11 03:30:00         4  \n2018-06-14 05:30:00         6  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>temp_C</th>\n      <th>rain_mm</th>\n      <th>wetness_percent</th>\n      <th>rel_humidity_percent</th>\n      <th>wind_speed_kmph</th>\n      <th>wind_direction</th>\n      <th>spray_drift_risk</th>\n      <th>backup_data</th>\n      <th>missing_data</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>start_hour</th>\n      <th>end_hour</th>\n    </tr>\n    <tr>\n      <th>datetime_mid</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-02-11 03:30:00</th>\n      <td>21.400000</td>\n      <td>0.000000</td>\n      <td>99.000000</td>\n      <td>100.000000</td>\n      <td>13.900000</td>\n      <td>NE</td>\n      <td>Best Conditions</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>2018</td>\n      <td>2</td>\n      <td>11</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2018-06-14 05:30:00</th>\n      <td>10.200000</td>\n      <td>0.000000</td>\n      <td>97.300000</td>\n      <td>99.200000</td>\n      <td>0.000000</td>\n      <td>SW</td>\n      <td>Not Recommended</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>2018</td>\n      <td>6</td>\n      <td>14</td>\n      <td>5</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checkpoint: save data\n",
    "The input weather data is now formatted. Output as csv and pickle file.\n",
    "The pickle file (.pkl) is a copy of the pre-processed DataFrame so you don't need to repeat the steps above.\n",
    "\n",
    "Import by using the code: `pd.read_pickle(file path to the pickle file)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "train_df.to_csv('../data/hourly_2016_2021.csv')\n",
    "train_df.to_pickle('../data/hourly_2016_2021.pkl')"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graphs\n",
    "Visual graph of temperature data. Data appears seasonal a high level (year and month level). Seasonality patterns are not as strong, and there are some irregularities when looking at temperature on the granular hourly level."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "total_temp_plot = train_df['temp_C'].plot(figsize=(16,5),title='Kumeu hourly temperature for Jan 2016 to May 2021',xlabel='year')"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "matplotlib is required for plotting when the default backend \"matplotlib\" is selected.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_104146/2529991176.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtotal_temp_plot\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_df\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'temp_C'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfigsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m16\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtitle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Kumeu hourly temperature for Jan 2016 to May 2021'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mxlabel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'year'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/kumeu-river-project/krw-prediction-modelling/venv/lib/python3.9/site-packages/pandas/plotting/_core.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    890\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    891\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 892\u001B[0;31m         \u001B[0mplot_backend\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_plot_backend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"backend\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    893\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    894\u001B[0m         x, y, kind, kwargs = self._get_call_args(\n",
      "\u001B[0;32m~/kumeu-river-project/krw-prediction-modelling/venv/lib/python3.9/site-packages/pandas/plotting/_core.py\u001B[0m in \u001B[0;36m_get_plot_backend\u001B[0;34m(backend)\u001B[0m\n\u001B[1;32m   1812\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_backends\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mbackend\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1813\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1814\u001B[0;31m     \u001B[0mmodule\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_load_backend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbackend\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1815\u001B[0m     \u001B[0m_backends\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mbackend\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1816\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/kumeu-river-project/krw-prediction-modelling/venv/lib/python3.9/site-packages/pandas/plotting/_core.py\u001B[0m in \u001B[0;36m_load_backend\u001B[0;34m(backend)\u001B[0m\n\u001B[1;32m   1752\u001B[0m             \u001B[0mmodule\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimportlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimport_module\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pandas.plotting._matplotlib\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1753\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mImportError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1754\u001B[0;31m             raise ImportError(\n\u001B[0m\u001B[1;32m   1755\u001B[0m                 \u001B[0;34m\"matplotlib is required for plotting when the \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1756\u001B[0m                 \u001B[0;34m'default backend \"matplotlib\" is selected.'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: matplotlib is required for plotting when the default backend \"matplotlib\" is selected."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "temp_2017 = train_df[['temp_C']].loc[train_df.index.year==2017].plot(figsize=(16,5),title='Kumeu hourly temperature 2017',xlabel='date')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "temp_1week = train_df[['temp_C']].loc[(train_df.index.year==2017) & (train_df.index.month==1) & (train_df.index.day<8)].plot(figsize=(16,5),title='Kumeu hourly temperature for 1st week of Jan 2017',xlabel='24 hour time')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "temp_1week = train_df[['temp_C']].loc[(train_df.index.year==2018) & (train_df.index.month==1) & (train_df.index.day<8)].plot(figsize=(16,5),title='Kumeu hourly temperature for 1st week of Jan 2018',xlabel='24 hour time')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing data for model training\n",
    "\n",
    "The data needs to be transformed before it can be input into a neural network model. The following steps are taken in this section:\n",
    "\n",
    "- One Hot Encoding\n",
    "- Standardised scaling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.drop(labels=['year','month','day','start_hour','end_hour','backup_data','spray_drift_risk','missing_data'], axis=1, inplace=True)\n",
    "\n",
    "train_df_encoded = pd.get_dummies(train_df, drop_first=True)\n",
    "#put temperature at the end of the dataframe because that will be the label - the value we are trying to predict\n",
    "train_df_encoded = train_df_encoded[['rain_mm','wetness_percent','rel_humidity_percent','wind_speed_kmph',\n",
    "                                    'wind_direction_N','wind_direction_NE','wind_direction_NW','wind_direction_S',\n",
    "                                    'wind_direction_SE','wind_direction_SW','wind_direction_W','temp_C']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df_encoded.sample(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df_encoded_scaled = pd.DataFrame(StandardScaler().fit_transform(train_df_encoded))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df_encoded_scaled.sample(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checkpoint: save data\n",
    "\n",
    "The input data is now formatted for neural network implementation. The data is saved as a csv file and also a pickle file. The pickle file can be loaded directly into a Python script so that you can skip all the above steps and start working directly with the data formatted for model implementation.\n",
    "\n",
    "Code to import pickle file:\n",
    "`pd.read_pickle(file path to the pickle file)`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df_encoded_scaled.to_csv('../kumeu-rainfall/data/hourly_2016_2021_encoded_scaled.csv')\n",
    "train_df_encoded_scaled.to_pickle('../kumeu-rainfall/data/hourly_2016_2021_encoded_scaled.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checkpoint: load data\n",
    "\n",
    "Load the numpy array that is prepared for neural network implementation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df_encoded_scaled = pd.read_pickle('../kumeu-rainfall/data/hourly_2016_2021_encoded_scaled.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Only train on temperature, humidity and wind speed\n",
    "#train_df_encoded_scaled =train_df_encoded_scaled[[11,2,3]]\n",
    "train_df_encoded_scaled = train_df_encoded_scaled.to_numpy()\n",
    "\n",
    "train_df_encoded_scaled"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#X = train_df_encoded_scaled[:,:-1]\n",
    "#y = train_df_encoded_scaled[:,-1]\n",
    "#X.shape, y.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train/test data split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=24, max_train_size=72, test_size=12)\n",
    "print(tscv)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "for train_index, test_index in tscv.split(train_df_encoded_scaled):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    trainX.append(train_df_encoded_scaled[train_index])\n",
    "    trainY.append(train_df_encoded_scaled[test_index])\n",
    "    #X_train, X_test = trainX[train_index], trainX[test_index]\n",
    "    #y_train, y_test = trainY[train_index], trainY[test_index]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# @author: Sreenivas Bhattiprolu\n",
    "\n",
    "# As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features. \n",
    "# In this example, the n_features is 2. We will make timesteps = 3. \n",
    "# With this, the resultant n_samples is 5 (as the input data has 9 rows).\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "n_future = 24  # Number of days we want to predict into the future\n",
    "n_past = 24  # Number of past days we want to use to predict the future\n",
    "\n",
    "# step=48\n",
    "# for i in range(0, len(train_df_encoded_scaled), step):\n",
    "#     mid_index = int((i + step)/2)\n",
    "#     end_index = int(i + step)\n",
    "#     trainX.append(train_df_encoded_scaled[i:mid_index,i:mid_index])\n",
    "#     trainY.append(train_df_encoded_scaled[mid_index:end_index,mid_index:end_index])\n",
    "\n",
    "for i in range(n_past, len(train_df_encoded_scaled) - n_future + 1):\n",
    "    trainX.append(train_df_encoded_scaled[i - n_past:i, 0:train_df_encoded_scaled.shape[1]])\n",
    "    trainY.append(train_df_encoded_scaled[i + n_future - 1:i + n_future, 0:train_df_encoded_scaled.shape[1]])\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "\n",
    "print('trainX shape: {}.'.format(trainX.shape))\n",
    "print('trainY shape: {}.'.format(trainY.shape))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define Autoencoder model\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding, Activation, RepeatVector, TimeDistributed\n",
    "\n",
    "model = Sequential(name='lstm')\n",
    "\n",
    "model.add(LSTM(trainX.shape[2], activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True, name = 'lstm1'))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=False, name='lstm2'))\n",
    "model.add(Dense(100, name='dense'))\n",
    "model.add(Activation(activation='relu', name='activation'))\n",
    "model.add(RepeatVector(n=24, name = 'repeat'))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True, name='lstm3'))\n",
    "model.add(TimeDistributed(Dense(trainX.shape[2]), name='timedistributed'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fit model\n",
    "history = model.fit(trainX, trainY, \n",
    "                    epochs=3,batch_size=64,\n",
    "                    validation_data=0.1, \n",
    "                    verbose=1)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Example LSTM - this is just an example, it doesn't forecast weather well\n",
    "# define Autoencoder model\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "# model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(trainY.shape[1]))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fit model\n",
    "# history = model.fit(trainX, trainY, \n",
    "#                     epochs=3, batch_size=24, \n",
    "#                     validation_data=0.1, \n",
    "#                     verbose=1)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "baf3f1d880254e75a31c5e94544c57485773e850702cfb93b52e6bf9787c84a1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}